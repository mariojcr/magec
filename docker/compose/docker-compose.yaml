# Magec - Fully local deployment
# Everything runs on your machine: LLM, STT, TTS, embeddings, memory.
# No API keys, no cloud accounts, no data leaving your network.
#
# GPU: Uncomment ONE of the GPU sections under ollama:
#   - Docker:  the 'deploy' block (uses nvidia container runtime)
#   - Podman:  the 'devices' line (uses CDI — requires nvidia-ctk cdi generate)
#
# Usage:
#   docker compose up -d
#   docker compose logs -f ollama-setup   # watch model downloads on first start

services:
  magec:
    image: ghcr.io/achetronic/magec:latest
    ports:
      - "8080:8080"
      - "8081:8081"
    volumes:
      - ./config.yaml:/app/config.yaml
      - magec_data:/app/data
    depends_on:
      ollama-setup:
        condition: service_completed_successfully
      redis:
        condition: service_started
      postgres:
        condition: service_started
      parakeet:
        condition: service_started
      tts:
        condition: service_started
    restart: unless-stopped

  redis:
    image: docker.io/library/redis:alpine
    volumes:
      - redis_data:/data
    restart: unless-stopped

  postgres:
    image: docker.io/pgvector/pgvector:pg17
    environment:
      POSTGRES_USER: magec
      POSTGRES_PASSWORD: magec
      POSTGRES_DB: magec
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  ollama:
    image: docker.io/ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # ── NVIDIA GPU acceleration (uncomment ONE option) ──────────────
    #
    # Option A — Docker (nvidia-container-runtime):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    #
    # Option B — Podman (CDI — run: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml):
    # devices:
    #   - nvidia.com/gpu=all

  ollama-setup:
    image: docker.io/ollama/ollama:latest
    depends_on:
      - ollama
    restart: "no"
    environment:
      OLLAMA_HOST: http://ollama:11434
    entrypoint: [""]
    command:
      - /bin/sh
      - -c
      - |
        echo "Waiting for Ollama to be ready..."
        until ollama list > /dev/null 2>&1; do
          sleep 2
        done
        echo "Pulling qwen3:8b (LLM)..."
        ollama pull qwen3:8b
        echo "Pulling nomic-embed-text (embeddings)..."
        ollama pull nomic-embed-text
        echo "Models ready."

  parakeet:
    image: ghcr.io/achetronic/parakeet:latest
    restart: unless-stopped

  tts:
    image: docker.io/travisvn/openai-edge-tts:latest
    environment:
      - REQUIRE_API_KEY=False
    restart: unless-stopped

volumes:
  magec_data:
  redis_data:
  postgres_data:
  ollama_data:
